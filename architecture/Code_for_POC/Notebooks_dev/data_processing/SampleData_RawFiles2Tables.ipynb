{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4864afcf",
   "metadata": {},
   "source": [
    "# Copy XLSX Files to Lakehouse Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdef79f",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ae949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Configuration\n",
    "SOURCE_FILES_PATH = \"Files/SalesLT_Raw/\"  # Folder containing Excel files\n",
    "TARGET_SCHEMA = \"SalesLT\"                 # Target schema for tables\n",
    "LOAD_TIMESTAMP = datetime.now().isoformat()\n",
    "LOAD_DATE = datetime.now().strftime(\"%Y-%m-%d\")  # Add missing LOAD_DATE\n",
    "\n",
    "# Expected SalesLT tables (matching Excel file names)\n",
    "EXPECTED_TABLES = [\n",
    "    'address', 'customer', 'customeraddress', 'product', \n",
    "    'productcategory', 'productdescription', 'productmodel',\n",
    "    'productmodelproductdescription', 'salesorderdetail', 'salesorderheader'\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Excel Files to SalesLT Schema Tables Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… Libraries imported\")\n",
    "print(f\"ðŸ“… Load Timestamp {LOAD_TIMESTAMP}\")\n",
    "print(f\"ðŸ“… Load Date: {LOAD_DATE}\")  # Show both for clarity\n",
    "print(f\"ðŸ“ Source files path: {SOURCE_FILES_PATH}\")\n",
    "print(f\"ðŸŽ¯ Target schema: {TARGET_SCHEMA}\")\n",
    "print(f\"ðŸ“‹ Expected tables: {len(EXPECTED_TABLES)}\")\n",
    "print(f\"âœ… Microsoft Fabric PySpark environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b215c",
   "metadata": {},
   "source": [
    "## Step 2: Discover Available Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Discover available Excel files\n",
    "print(\"ðŸ” DISCOVERING EXCEL FILES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "    \n",
    "    print(f\"ðŸ“ Target directory: {SOURCE_FILES_PATH}\")\n",
    "    \n",
    "    # Direct approach - try the configured path\n",
    "    file_list = mssparkutils.fs.ls(SOURCE_FILES_PATH)\n",
    "    excel_files = [f for f in file_list if f.name.endswith('.xlsx')]\n",
    "    \n",
    "    print(f\"âœ… Found {len(excel_files)} Excel files in {SOURCE_FILES_PATH}\")\n",
    "    \n",
    "    if len(excel_files) > 0:\n",
    "        print(f\"\\nðŸ“‹ Excel files found:\")\n",
    "        \n",
    "        files_to_process = []\n",
    "        for file_info in excel_files:\n",
    "            file_name = file_info.name\n",
    "            table_name = file_name.replace('.xlsx', '').lower()\n",
    "            \n",
    "            is_expected = table_name in [t.lower() for t in EXPECTED_TABLES]\n",
    "            marker = \"ðŸŽ¯\" if is_expected else \"ðŸ“‹\"\n",
    "            print(f\"   {marker} {file_name} â†’ {table_name}\")\n",
    "            \n",
    "            if is_expected:\n",
    "                files_to_process.append({\n",
    "                    'file_name': file_name,\n",
    "                    'file_path': file_info.path,\n",
    "                    'table_name': table_name\n",
    "                })\n",
    "        \n",
    "        FILES_TO_PROCESS = files_to_process\n",
    "        print(f\"\\nðŸŽ‰ Ready to process {len(files_to_process)} matching files!\")\n",
    "        \n",
    "        if len(files_to_process) < len(excel_files):\n",
    "            print(f\"âš ï¸ {len(excel_files) - len(files_to_process)} files don't match expected names\")\n",
    "    else:\n",
    "        FILES_TO_PROCESS = []\n",
    "        print(f\"\\nâŒ No Excel files found in {SOURCE_FILES_PATH}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error accessing {SOURCE_FILES_PATH}: {str(e)}\")\n",
    "    print(\"\\nðŸ”§ DIAGNOSTIC: Let's check what's available...\")\n",
    "    \n",
    "    # Simple diagnostic if the main path fails\n",
    "    try:\n",
    "        print(f\"ðŸ“ Checking Files/ directory:\")\n",
    "        files_root = mssparkutils.fs.ls(\"Files/\")\n",
    "        for item in files_root:\n",
    "            item_type = \"ðŸ“\" if item.isDir else \"ðŸ“„\"\n",
    "            print(f\"   {item_type} {item.name}\")\n",
    "    except Exception as diag_error:\n",
    "        print(f\"âŒ Cannot access Files/ directory: {str(diag_error)}\")\n",
    "    \n",
    "    FILES_TO_PROCESS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851bfd8",
   "metadata": {},
   "source": [
    "# STEP 3: COPY Fils to Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60041dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Copy Excel files to SalesLT schema tables\n",
    "print(\"ðŸš€ PROCESSING EXCEL FILES TO SALESLT TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'FILES_TO_PROCESS' not in locals() or len(FILES_TO_PROCESS) == 0:\n",
    "    print(\"âŒ No files to process. Run previous steps first.\")\n",
    "else:\n",
    "    print(f\"ðŸ“‹ Processing {len(FILES_TO_PROCESS)} Excel files\")\n",
    "    print(f\"ðŸ“ Source: {SOURCE_FILES_PATH}\")\n",
    "    print(f\"ðŸŽ¯ Target schema: {TARGET_SCHEMA}\")\n",
    "    print(f\"ðŸ“… Load date: {LOAD_DATE}\")\n",
    "    print()\n",
    "    \n",
    "    # Processing results tracking\n",
    "    results = []\n",
    "    total_rows_processed = 0\n",
    "    \n",
    "    for i, file_info in enumerate(FILES_TO_PROCESS, 1):\n",
    "        file_name = file_info['file_name']\n",
    "        file_path = file_info['file_path']\n",
    "        table_name = file_info['table_name']\n",
    "        \n",
    "        print(f\"[{i}/{len(FILES_TO_PROCESS)}] Processing {file_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read Excel file using Spark directly instead of pandas path conversion\n",
    "            print(f\"   ðŸ“– Reading Excel file: {file_name}\")\n",
    "            \n",
    "            # Use Spark to read the Excel file directly\n",
    "            # First, let's try using the Fabric file path directly\n",
    "            spark_file_path = f\"/lakehouse/default/Files/SalesLT_Raw/{file_name}\"\n",
    "            \n",
    "            # Read Excel using pandas with the correct Fabric path\n",
    "            pandas_df = pd.read_excel(spark_file_path)\n",
    "            row_count = len(pandas_df)\n",
    "            \n",
    "            print(f\"   âœ… Excel data loaded: {row_count:,} rows, {len(pandas_df.columns)} columns\")\n",
    "            \n",
    "            # Convert pandas DataFrame to Spark DataFrame\n",
    "            print(f\"   ðŸ”„ Converting to Spark DataFrame...\")\n",
    "            spark_df = spark.createDataFrame(pandas_df)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            print(f\"   ðŸ·ï¸ Adding metadata columns...\")\n",
    "            enriched_df = spark_df \\\n",
    "                .withColumn(\"_load_date\", lit(LOAD_DATE)) \\\n",
    "                .withColumn(\"_load_timestamp\", lit(LOAD_TIMESTAMP)) \\\n",
    "                .withColumn(\"_source_file\", lit(file_name)) \\\n",
    "                .withColumn(\"_source_path\", lit(file_path)) \\\n",
    "                .withColumn(\"_processing_timestamp\", current_timestamp()) \\\n",
    "                .withColumn(\"_load_method\", lit(\"excel_file_import\")) \\\n",
    "                .withColumn(\"_record_source\", lit(\"sample_data_files\"))\n",
    "            \n",
    "            # Create/replace table in SalesLT schema\n",
    "            target_table = f\"{TARGET_SCHEMA}.{table_name}\"\n",
    "            print(f\"   ðŸ¢ Creating table: {target_table}\")\n",
    "            \n",
    "            enriched_df.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(target_table)\n",
    "            \n",
    "            print(f\"   âœ… Table created successfully: {row_count:,} rows\")\n",
    "            \n",
    "            # Success tracking\n",
    "            total_rows_processed += row_count\n",
    "            results.append({\n",
    "                \"file\": file_name,\n",
    "                \"table\": target_table,\n",
    "                \"rows\": row_count,\n",
    "                \"columns\": len(pandas_df.columns),\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "            \n",
    "            print(f\"   ðŸŽ‰ Successfully processed {row_count:,} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:100]\n",
    "            results.append({\n",
    "                \"file\": file_name,\n",
    "                \"table\": f\"{TARGET_SCHEMA}.{table_name}\",\n",
    "                \"rows\": 0,\n",
    "                \"columns\": 0,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg\n",
    "            })\n",
    "            print(f\"   âŒ Failed: {error_msg}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Processing summary\n",
    "    successful = [r for r in results if r[\"status\"] == \"success\"]\n",
    "    failed = [r for r in results if r[\"status\"] == \"failed\"]\n",
    "    \n",
    "    print(\"ðŸŽ‰ PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… Successfully processed: {len(successful)} files\")\n",
    "    print(f\"âŒ Failed processing: {len(failed)} files\")\n",
    "    print(f\"ðŸ“Š Total rows processed: {total_rows_processed:,}\")\n",
    "    print(f\"ðŸ“… Processing completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if successful:\n",
    "        print(f\"\\nðŸ¢ Created SalesLT tables:\")\n",
    "        for result in successful:\n",
    "            print(f\"âœ… {result['table']}: {result['rows']:,} rows, {result['columns']} columns\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nâš ï¸ Processing failures:\")\n",
    "        for result in failed:\n",
    "            print(f\"âŒ {result['file']} â†’ {result['table']}: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ SalesLT schema tables ready!\")\n",
    "    print(f\"ðŸ’¡ Tables can be queried using: SELECT * FROM {TARGET_SCHEMA}.[tablename]\")\n",
    "    \n",
    "    # Quick verification - check if tables are accessible\n",
    "    print(f\"\\nðŸ” QUICK VERIFICATION:\")\n",
    "    try:\n",
    "        # Try to query one of the created tables\n",
    "        if successful:\n",
    "            test_table = successful[0]['table']\n",
    "            test_df = spark.sql(f\"SELECT COUNT(*) as row_count FROM {test_table}\")\n",
    "            test_count = test_df.collect()[0]['row_count']\n",
    "            print(f\"âœ… Verified: {test_table} contains {test_count:,} rows\")\n",
    "            \n",
    "            # Show all tables in SalesLT schema\n",
    "            print(f\"\\nðŸ“‹ Tables in {TARGET_SCHEMA} schema:\")\n",
    "            schema_tables = spark.sql(f\"SHOW TABLES IN {TARGET_SCHEMA}\").collect()\n",
    "            for table_row in schema_tables:\n",
    "                print(f\"   ðŸ¢ {table_row.tableName}\")\n",
    "        else:\n",
    "            print(\"âŒ No successful tables to verify\")\n",
    "            \n",
    "    except Exception as verify_error:\n",
    "        print(f\"âš ï¸ Verification failed: {str(verify_error)[:80]}...\")\n",
    "        print(\"ðŸ’¡ Tables might be in default schema - try: SHOW TABLES\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
