{"cells":[{"cell_type":"markdown","source":["# Load Bronze Data to Silver Table - Payment (Fabric & ADB)\n","\n","## Overview\n","Load Payment sample data from Bronze lakehouse files into Silver lakehouse table for both Fabric and ADB channels.\n","\n","## Data Flow\n","- **Source (Fabric)**: MAAG_LH_Bronze/Files/samples_fabric/finance/Payment_Samples_Fabric.csv\n","- **Source (ADB)**: MAAG_LH_Bronze/Files/samples_databricks/finance/Payment_Samples_ADB.csv\n","- **Target**: MAAG_LH_Silver.finance.Payment table (or any attached default lakehouse)\n","- **Process**: Read CSV, validate schema, check data quality, show value distributions, load to Delta table, verify load\n","\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c97b4ced-e78d-429f-be1f-066dfdcf5298"},{"cell_type":"code","source":["# --- Fabric Channel ---\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import col, sum as spark_sum\n","from pyspark.sql import functions as F\n","\n","WORKSPACE_NAME = \"Fabric_MAAG\"\n","SOURCE_LAKEHOUSE_NAME = \"MAAG_LH_Bronze\"\n","FABRIC_SOURCE_PATH = f\"abfss://{WORKSPACE_NAME}@onelake.dfs.fabric.microsoft.com/{SOURCE_LAKEHOUSE_NAME}.Lakehouse/Files/samples_fabric/finance/Payment_Samples_Fabric.csv\"\n","\n","TARGET_SCHEMA = \"finance\"\n","TARGET_TABLE = \"Payment\"\n","TARGET_FULL_PATH = f\"{TARGET_SCHEMA}.{TARGET_TABLE}\"\n","\n","print(f\"🔄 Loading Fabric Payment data\")\n","print(f\"📂 Source: {FABRIC_SOURCE_PATH}\")\n","print(f\"🎯 Target: {TARGET_FULL_PATH}\")\n","\n","# Read CSV from Bronze lakehouse\n","payment_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(FABRIC_SOURCE_PATH)\n","\n","print(f\"✅ Data loaded successfully\")\n","print(f\"📊 Records: {payment_df.count()}\")\n","print(f\"📋 Columns: {payment_df.columns}\")\n","\n","# Display sample data\n","print(f\"\\n📖 Sample data:\")\n","payment_df.show(10, truncate=False)\n","\n","required_columns = [\n","    \"PaymentId\", \"PaymentNumber\", \"InvoiceId\", \"OrderId\", \"PaymentDate\", \"PaymentAmount\",\n","    \"PaymentStatus\", \"PaymentMethod\", \"CreatedBy\"\n"," ]\n","\n","missing_columns = [c for c in required_columns if c not in payment_df.columns]\n","if missing_columns:\n","    print(f\"⚠️ Warning: Missing columns in source data: {missing_columns}\")\n","else:\n","    print(f\"✅ All required columns present in source data.\")\n","\n","for col_name in missing_columns:\n","    if col_name == \"PaymentAmount\":\n","        payment_df = payment_df.withColumn(col_name, F.lit(0.0))\n","    else:\n","        payment_df = payment_df.withColumn(col_name, F.lit(\"\"))\n","\n","payment_df = payment_df.withColumn(\"PaymentId\", col(\"PaymentId\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"PaymentNumber\", col(\"PaymentNumber\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"InvoiceId\", col(\"InvoiceId\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"OrderId\", col(\"OrderId\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"PaymentDate\", col(\"PaymentDate\").cast(DateType()))\n","payment_df = payment_df.withColumn(\"PaymentAmount\", col(\"PaymentAmount\").cast(DoubleType()))\n","payment_df = payment_df.withColumn(\"PaymentStatus\", col(\"PaymentStatus\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"PaymentMethod\", col(\"PaymentMethod\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"CreatedBy\", col(\"CreatedBy\").cast(StringType()))\n","payment_df = payment_df.select(required_columns)\n","\n","# Data quality checks\n","print(f\"\\n📊 Data Quality Check:\")\n","null_counts = payment_df.select([F.sum(col(c).isNull().cast(\"int\")).alias(c) for c in required_columns]).collect()[0]\n","for col_name in required_columns:\n","    null_count = null_counts[col_name]\n","    if null_count > 0:\n","        print(f\"  {col_name}: {null_count} null values\")\n","    else:\n","        print(f\"  {col_name}: ✅ No nulls\")\n","\n","# Show value distributions for PaymentStatus\n","print(f\"\\n🎯 PaymentStatus Distribution:\")\n","payment_df.groupBy(\"PaymentStatus\").count().orderBy(\"PaymentStatus\").show()\n","\n","# Ensure the target schema exists\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_SCHEMA}\")\n","\n","# Load data to Silver table\n","print(f\"💾 Loading data to Silver table: {TARGET_FULL_PATH}\")\n","try:\n","    payment_df.write \\\n","      .format(\"delta\") \\\n","      .mode(\"append\") \\\n","      .option(\"overwriteSchema\", \"true\") \\\n","      .saveAsTable(TARGET_FULL_PATH)\n","    print(f\"✅ Data loaded successfully to {TARGET_FULL_PATH}\")\n","    # Verify the load\n","    result_count = spark.sql(f\"SELECT COUNT(*) as count FROM {TARGET_FULL_PATH}\").collect()[0][\"count\"]\n","    print(f\"📊 Records in target table: {result_count}\")\n","    # Show sample of loaded data\n","    print(f\"\\n📖 Sample from Silver table:\")\n","    spark.sql(f\"SELECT * FROM {TARGET_FULL_PATH} ORDER BY PaymentId\").show(10, truncate=False)\n","    print(f\"🎉 Payment data load complete!\")\n","except Exception as e:\n","    print(f\"❌ Error loading data to table: {str(e)}\")\n","    raise\n","# --- End Fabric Channel ---"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"20ddaee1-ec73-4388-ac20-f47963819a56","normalized_state":"finished","queued_time":"2025-08-25T13:14:14.2197162Z","session_start_time":"2025-08-25T13:14:14.2208632Z","execution_start_time":"2025-08-25T13:14:24.8578363Z","execution_finish_time":"2025-08-25T13:15:00.2639621Z","parent_msg_id":"2c82b3f4-fdec-4af7-9fb6-33613b6c4cd2"},"text/plain":"StatementMeta(, 20ddaee1-ec73-4388-ac20-f47963819a56, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🔄 Loading Fabric Payment data\n📂 Source: abfss://Fabric_MAAG@onelake.dfs.fabric.microsoft.com/MAAG_LH_Bronze.Lakehouse/Files/samples_fabric/finance/Payment_Samples_Fabric.csv\n🎯 Target: finance.Payment\n✅ Data loaded successfully\n📊 Records: 1808\n📋 Columns: ['PaymentId', 'PaymentNumber', 'InvoiceId', 'CustomerId', 'PaymentDate', 'PaymentAmount', 'PaymentMethod', 'PaymentStatus']\n\n📖 Sample data:\n+------------------------------------+-------------+------------------------------------+----------+-----------+-------------+-------------+-------------+\n|PaymentId                           |PaymentNumber|InvoiceId                           |CustomerId|PaymentDate|PaymentAmount|PaymentMethod|PaymentStatus|\n+------------------------------------+-------------+------------------------------------+----------+-----------+-------------+-------------+-------------+\n|7c82dbb4-99bb-4f83-a7d6-f9aa62282c1a|PM-F100000   |547fb079-2c37-456d-b7fc-e0a081bbf04f|CID-001   |2024-03-05 |16696.36     |MC           |Completed    |\n|97240d28-bf79-4ef0-b5de-9cfc733ab033|PM-F100001   |debf63a9-3399-4fcd-bf3c-1c6fd956d2e5|CID-001   |2021-12-22 |9725.93      |MC           |Completed    |\n|4e791111-3eae-4034-88b6-184329272759|PM-F100002   |2e1730d6-9db4-491e-95fc-bbff20cd5a92|CID-001   |2024-04-09 |306.48       |PayPal       |Completed    |\n|58b34c96-117e-4343-9cbc-62a1badeaba5|PM-F100003   |22a36d89-f333-43f3-b640-13de22c530cc|CID-001   |2020-07-19 |761.99       |PayPal       |Completed    |\n|96d1f367-11f8-4b1a-be5a-580155b46d76|PM-F100004   |9792ed6f-71ae-40cc-8efb-bb67ffc6fe01|CID-001   |2022-12-07 |14613.02     |MC           |Completed    |\n|fe6e7b33-0e91-4a98-808a-4f0ab212ca00|PM-F100005   |585d89fb-30e0-434b-afee-7d6d80c8ce6c|CID-001   |2022-12-21 |2408.87      |VISA         |Completed    |\n|56a72cee-18b2-4a79-a00d-4d571a7ff0cb|PM-F100006   |e5a23120-3462-469d-af56-8462346fde95|CID-002   |2021-11-08 |7134.29      |Discover     |Completed    |\n|f71bcd3f-b4f2-4842-af1d-95a3e1003b6b|PM-F100007   |d01dd98a-fa6f-4b29-a251-4e2a17f87977|CID-002   |2025-05-29 |12534.54     |MC           |Completed    |\n|9bf1ab7d-f972-438a-bbea-59ab9823a01d|PM-F100008   |e437c85a-4f4e-4105-8dec-faf2d69b6166|CID-003   |2024-02-19 |786.02       |MC           |Completed    |\n|57de6c77-81fa-41c4-aa3f-70d075ac334f|PM-F100009   |05fa06e9-024b-4582-921e-e6598be87b38|CID-003   |2022-10-21 |2143.77      |MC           |Completed    |\n+------------------------------------+-------------+------------------------------------+----------+-----------+-------------+-------------+-------------+\nonly showing top 10 rows\n\n⚠️ Warning: Missing columns in source data: ['OrderId', 'CreatedBy']\n\n📊 Data Quality Check:\n  PaymentId: ✅ No nulls\n  PaymentNumber: ✅ No nulls\n  InvoiceId: ✅ No nulls\n  OrderId: ✅ No nulls\n  PaymentDate: ✅ No nulls\n  PaymentAmount: ✅ No nulls\n  PaymentStatus: ✅ No nulls\n  PaymentMethod: ✅ No nulls\n  CreatedBy: ✅ No nulls\n\n🎯 PaymentStatus Distribution:\n+-------------+-----+\n|PaymentStatus|count|\n+-------------+-----+\n|    Completed| 1543|\n|       Failed|   78|\n|      Pending|  187|\n+-------------+-----+\n\n💾 Loading data to Silver table: finance.Payment\n✅ Data loaded successfully to finance.Payment\n📊 Records in target table: 3616\n\n📖 Sample from Silver table:\n+------------------------------------+-------------+------------------------------------+-------+-----------+-------------+-------------+-------------+---------+\n|PaymentId                           |PaymentNumber|InvoiceId                           |OrderId|PaymentDate|PaymentAmount|PaymentStatus|PaymentMethod|CreatedBy|\n+------------------------------------+-------------+------------------------------------+-------+-----------+-------------+-------------+-------------+---------+\n|00099c6b-7a82-4d8f-92da-a240f6a7c938|PM-F100053   |ef8fedbd-4650-4727-b274-60bc47f9e0a6|       |2019-09-01 |22182.5      |Completed    |MC           |         |\n|00099c6b-7a82-4d8f-92da-a240f6a7c938|PM-F100053   |ef8fedbd-4650-4727-b274-60bc47f9e0a6|       |2019-09-01 |22182.5      |Completed    |MC           |         |\n|00487a1a-1cef-4e08-8fe5-34dda7d93a75|PM-F101462   |cdf407d4-92d2-4834-8c07-ad4565c9d07a|       |2022-03-07 |2149.32      |Failed       |VISA         |         |\n|00487a1a-1cef-4e08-8fe5-34dda7d93a75|PM-F101462   |cdf407d4-92d2-4834-8c07-ad4565c9d07a|       |2022-03-07 |2149.32      |Failed       |VISA         |         |\n|0048897a-669d-4b2c-9c6d-55c021798d11|PM-F100689   |b7594e1a-8cd8-4d58-bd0a-091a53513d93|       |2023-08-25 |638.89       |Completed    |VISA         |         |\n|0048897a-669d-4b2c-9c6d-55c021798d11|PM-F100689   |b7594e1a-8cd8-4d58-bd0a-091a53513d93|       |2023-08-25 |638.89       |Completed    |VISA         |         |\n|004924d4-783d-4cce-9d8e-75e1f29a12d0|PM-F101438   |5a0323c7-5593-4744-a466-e4094216a775|       |2022-02-09 |1617.32      |Completed    |MC           |         |\n|004924d4-783d-4cce-9d8e-75e1f29a12d0|PM-F101438   |5a0323c7-5593-4744-a466-e4094216a775|       |2022-02-09 |1617.32      |Completed    |MC           |         |\n|006a202d-0635-4b67-a383-9b687b99212f|PM-F100670   |9927f355-b4ab-4f30-8900-dde901aa99bc|       |2024-08-12 |2853.3       |Completed    |VISA         |         |\n|006a202d-0635-4b67-a383-9b687b99212f|PM-F100670   |9927f355-b4ab-4f30-8900-dde901aa99bc|       |2024-08-12 |2853.3       |Completed    |VISA         |         |\n+------------------------------------+-------------+------------------------------------+-------+-----------+-------------+-------------+-------------+---------+\nonly showing top 10 rows\n\n🎉 Payment data load complete!\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20dbb72c-079a-42c9-8730-d952a43bf916"},{"cell_type":"code","source":["# --- ADB Channel ---\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import col, sum as spark_sum\n","from pyspark.sql import functions as F\n","\n","WORKSPACE_NAME = \"Fabric_MAAG\"\n","SOURCE_LAKEHOUSE_NAME = \"MAAG_LH_Bronze\"\n","ADB_SOURCE_PATH = f\"abfss://{WORKSPACE_NAME}@onelake.dfs.fabric.microsoft.com/{SOURCE_LAKEHOUSE_NAME}.Lakehouse/Files/samples_databricks/finance/Payment_Samples_ADB.csv\"\n","\n","TARGET_SCHEMA = \"finance\"\n","TARGET_TABLE = \"Payment\"\n","TARGET_FULL_PATH = f\"{TARGET_SCHEMA}.{TARGET_TABLE}\"\n","\n","print(f\"🔄 Loading ADB Payment data\")\n","print(f\"📂 Source: {ADB_SOURCE_PATH}\")\n","print(f\"🎯 Target: {TARGET_FULL_PATH}\")\n","\n","# Read CSV from Bronze lakehouse\n","payment_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(ADB_SOURCE_PATH)\n","\n","print(f\"✅ Data loaded successfully\")\n","print(f\"📊 Records: {payment_df.count()}\")\n","print(f\"📋 Columns: {payment_df.columns}\")\n","\n","# Display sample data\n","print(f\"\\n📖 Sample data:\")\n","payment_df.show(10, truncate=False)\n","\n","required_columns = [\n","    \"PaymentId\", \"PaymentNumber\", \"InvoiceId\", \"OrderId\", \"PaymentDate\", \"PaymentAmount\",\n","    \"PaymentStatus\", \"PaymentMethod\", \"CreatedBy\"\n"," ]\n","\n","missing_columns = [c for c in required_columns if c not in payment_df.columns]\n","if missing_columns:\n","    print(f\"⚠️ Warning: Missing columns in source data: {missing_columns}\")\n","else:\n","    print(f\"✅ All required columns present in source data.\")\n","\n","for col_name in missing_columns:\n","    if col_name == \"PaymentAmount\":\n","        payment_df = payment_df.withColumn(col_name, F.lit(0.0))\n","    else:\n","        payment_df = payment_df.withColumn(col_name, F.lit(\"\"))\n","\n","payment_df = payment_df.withColumn(\"PaymentId\", col(\"PaymentId\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"PaymentNumber\", col(\"PaymentNumber\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"InvoiceId\", col(\"InvoiceId\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"OrderId\", col(\"OrderId\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"PaymentDate\", col(\"PaymentDate\").cast(DateType()))\n","payment_df = payment_df.withColumn(\"PaymentAmount\", col(\"PaymentAmount\").cast(DoubleType()))\n","payment_df = payment_df.withColumn(\"PaymentStatus\", col(\"PaymentStatus\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"PaymentMethod\", col(\"PaymentMethod\").cast(StringType()))\n","payment_df = payment_df.withColumn(\"CreatedBy\", col(\"CreatedBy\").cast(StringType()))\n","payment_df = payment_df.select(required_columns)\n","\n","# Data quality checks\n","print(f\"\\n📊 Data Quality Check:\")\n","null_counts = payment_df.select([F.sum(col(c).isNull().cast(\"int\")).alias(c) for c in required_columns]).collect()[0]\n","for col_name in required_columns:\n","    null_count = null_counts[col_name]\n","    if null_count > 0:\n","        print(f\"  {col_name}: {null_count} null values\")\n","    else:\n","        print(f\"  {col_name}: ✅ No nulls\")\n","\n","# Show value distributions for PaymentStatus\n","print(f\"\\n🎯 PaymentStatus Distribution:\")\n","payment_df.groupBy(\"PaymentStatus\").count().orderBy(\"PaymentStatus\").show()\n","\n","# Ensure the target schema exists\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_SCHEMA}\")\n","\n","# Load data to Silver table\n","print(f\"💾 Loading data to Silver table: {TARGET_FULL_PATH}\")\n","try:\n","    payment_df.write \\\n","      .format(\"delta\") \\\n","      .mode(\"append\") \\\n","      .option(\"overwriteSchema\", \"true\") \\\n","      .saveAsTable(TARGET_FULL_PATH)\n","    print(f\"✅ Data loaded successfully to {TARGET_FULL_PATH}\")\n","    # Verify the load\n","    result_count = spark.sql(f\"SELECT COUNT(*) as count FROM {TARGET_FULL_PATH}\").collect()[0][\"count\"]\n","    print(f\"📊 Records in target table: {result_count}\")\n","    # Show sample of loaded data\n","    print(f\"\\n📖 Sample from Silver table:\")\n","    spark.sql(f\"SELECT * FROM {TARGET_FULL_PATH} ORDER BY PaymentId\").show(10, truncate=False)\n","    print(f\"🎉 Payment data load complete!\")\n","except Exception as e:\n","    print(f\"❌ Error loading data to table: {str(e)}\")\n","    raise\n","# --- End ADB Channel ---"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":null,"normalized_state":"session_error","queued_time":"2025-08-25T13:08:41.3282107Z","session_start_time":"2025-08-25T13:08:41.3292278Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"f09f4f94-6104-4a71-a537-42c5200e4a50"},"text/plain":"StatementMeta(, , -1, SessionError, , SessionError)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","traceback":["InvalidHttpRequestToLivy: [TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd599fd9-dadd-48e5-85c7-3e3670fe3e96"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"1acd48dc-8b9d-4f55-ae21-a60a67b20bf4"}],"default_lakehouse":"1acd48dc-8b9d-4f55-ae21-a60a67b20bf4","default_lakehouse_name":"MAAG_LH_Silver","default_lakehouse_workspace_id":"e1e9b2bb-4338-4ab7-86fc-28098538ede0"}}},"nbformat":4,"nbformat_minor":5}