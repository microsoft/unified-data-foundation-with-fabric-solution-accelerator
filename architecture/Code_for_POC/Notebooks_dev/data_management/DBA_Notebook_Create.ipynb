{"cells":[{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29e3ab9a-6dca-41cc-9f29-54bad59b1da1"},{"cell_type":"code","source":["\n","# Configuration\n","SCHEMA_NAME = \"shared\"\n","TABLE_NAME = \"product_test\"\n","\n","\n","#. Create Product table\n","print(f\"ðŸ”¨ Creating Product table...\")\n","create_table_sql = f\"\"\"\n","CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{TABLE_NAME} (\n","    ProductId STRING,\n","    ProductName STRING,\n","    ProductDescription STRING,\n","    BrandName STRING,\n","    ProductNumber STRING,\n","    Color STRING,\n","    ProductModel STRING,\n","    ProductCategoryID STRING,\n","    CategoryName STRING,\n","    ListPrice DECIMAL(18,2),\n","    StandardCost DECIMAL(18,2),\n","    Weight DECIMAL(18,3),\n","    WeightUom STRING,     -- kg, lb, oz\n","    ProductStatus STRING, -- active, inactive, discontinued\n","    CreatedDate DATE,\n","    SellStartDate DATE,\n","    SellEndDate DATE,\n","    IsoCurrencyCode STRING,\n","    UpdatedDate DATE,\n","    CreatedBy STRING,\n","    UpdatedBy STRING\n",")\n","USING DELTA\n","\"\"\"\n","spark.sql(create_table_sql)\n","print(f\"âœ… {SCHEMA_NAME}.{TABLE_NAME} table created!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":null,"normalized_state":"session_error","queued_time":"2025-08-18T21:43:11.1514722Z","session_start_time":"2025-08-18T21:43:11.1525172Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"9a2717ba-b733-457a-b7c9-593e84ef9be5"},"text/plain":"StatementMeta(, , -1, SessionError, , SessionError)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","traceback":["InvalidHttpRequestToLivy: [TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b399ec6-b902-495c-9887-d6d98c72c23a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"0bb08fa5-8a0c-4303-b472-c5c489ef95b8"}],"default_lakehouse":"0bb08fa5-8a0c-4303-b472-c5c489ef95b8","default_lakehouse_name":"MAAG_LH_Gold","default_lakehouse_workspace_id":"e1e9b2bb-4338-4ab7-86fc-28098538ede0"}}},"nbformat":4,"nbformat_minor":5}