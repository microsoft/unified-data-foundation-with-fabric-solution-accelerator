{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b446bf",
   "metadata": {},
   "source": [
    "# Load Bronze Data to ADB Table - Orders (ADB Channel)\n",
    "\n",
    "## Overview\n",
    "Load Order sample data from Bronze lakehouse CSV file into Silver table in Databricks workspace catalog.\n",
    "\n",
    "## Data Flow\n",
    "- **Source (CSV)**: `DBFS sales/Order_Samples_ADB.csv`\n",
    "- **Target**: Workspace catalog `sales.order`\n",
    "- **Process**: Read CSV, validate schema, check data quality, load to Delta table, verify load\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for schema, table name, and file path\n",
    "# Widgets (so you can configure from UI/job)\n",
    "dbutils.widgets.text(\"catalog_name\", \"maag_adb2\")\n",
    "dbutils.widgets.text(\"schema_name\", \"sales\")\n",
    "dbutils.widgets.text(\"base_path\", \"/FileStore/tables/sales\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "table_order = 'order'\n",
    "order_csv = f'{base_path}/Order_Samples_ADB.csv'\n",
    "target_full_path = f'{catalog_name}.{schema_name}.{table_order}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check CSV file existence in DBFS\n",
    "print(f\"Checking files in base_path: {base_path}\")\n",
    "dbutils.fs.ls(base_path)\n",
    "print(f\"Looking for file: {order_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DataFrame\n",
    "print(f'🔄 Loading Order data')\n",
    "print(f'📂 Source: {base_path}')\n",
    "print(f'📂 Source: {order_csv}')\n",
    "print(f'🎯 Target: {target_full_path}')\n",
    "\n",
    "order_df = spark.read.option('header', True).option('inferSchema', True).csv(order_csv)\n",
    "\n",
    "print(f'✅ Data loaded successfully')\n",
    "print(f'📊 Records: {order_df.count()}')\n",
    "print(f'📋 Columns: {order_df.columns}')\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\n📖 Sample data:\")\n",
    "order_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and confirm to target schema\n",
    "print(f'🔍 Validating data quality...')\n",
    "\n",
    "required_columns = [\n",
    "    'OrderId', 'SalesChannelId', 'OrderNumber', 'CustomerId', 'CustomerAccountId',\n",
    "    'OrderDate', 'OrderStatus', 'SubTotal', 'TaxAmount', 'OrderTotal',\n",
    "    'PaymentMethod', 'IsoCurrencyCode', 'CreatedBy'\n",
    "]\n",
    "\n",
    "missing_columns = [c for c in required_columns if c not in order_df.columns]\n",
    "if missing_columns:\n",
    "    print(f'⚠️ Warning: Missing columns in source data: {missing_columns}')\n",
    "else:\n",
    "    print(f'✅ All required columns present in source data.')\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "for col_name in missing_columns:\n",
    "    if col_name in ['SubTotal', 'TaxAmount', 'OrderTotal']:\n",
    "        order_df = order_df.withColumn(col_name, F.lit(0.0))\n",
    "    elif col_name == 'CreatedBy':\n",
    "        order_df = order_df.withColumn(col_name, F.lit('Script'))\n",
    "    else:\n",
    "        order_df = order_df.withColumn(col_name, F.lit(''))\n",
    "\n",
    "from pyspark.sql.types import StringType, DoubleType, DateType\n",
    "order_df = order_df.withColumn('OrderId', col('OrderId').cast(StringType()))\n",
    "order_df = order_df.withColumn('SalesChannelId', col('SalesChannelId').cast(StringType()))\n",
    "order_df = order_df.withColumn('OrderNumber', col('OrderNumber').cast(StringType()))\n",
    "order_df = order_df.withColumn('CustomerId', col('CustomerId').cast(StringType()))\n",
    "order_df = order_df.withColumn('CustomerAccountId', col('CustomerAccountId').cast(StringType()))\n",
    "order_df = order_df.withColumn('OrderDate', col('OrderDate').cast(DateType()))\n",
    "order_df = order_df.withColumn('OrderStatus', col('OrderStatus').cast(StringType()))\n",
    "order_df = order_df.withColumn('SubTotal', col('SubTotal').cast(DoubleType()))\n",
    "order_df = order_df.withColumn('TaxAmount', col('TaxAmount').cast(DoubleType()))\n",
    "order_df = order_df.withColumn('OrderTotal', col('OrderTotal').cast(DoubleType()))\n",
    "order_df = order_df.withColumn('PaymentMethod', col('PaymentMethod').cast(StringType()))\n",
    "order_df = order_df.withColumn('IsoCurrencyCode', col('IsoCurrencyCode').cast(StringType()))\n",
    "order_df = order_df.withColumn('CreatedBy', F.when(col('CreatedBy').isNull() | (col('CreatedBy') == ''), 'Script').otherwise(col('CreatedBy')).cast(StringType()))\n",
    "order_df = order_df.select(required_columns)\n",
    "\n",
    "print(f'\\n📊 Data Quality Check:')\n",
    "null_counts = order_df.select([F.sum(col(c).isNull().cast('int')).alias(c) for c in required_columns]).collect()[0]\n",
    "for col_name in required_columns:\n",
    "    null_count = null_counts[col_name]\n",
    "    if null_count > 0:\n",
    "        print(f'  {col_name}: {null_count} null values')\n",
    "    else:\n",
    "        print(f'  {col_name}: ✅ No nulls')\n",
    "\n",
    "print(f'\\n🎯 OrderStatus Distribution:')\n",
    "order_df.groupBy('OrderStatus').count().orderBy('OrderStatus').show()\n",
    "\n",
    "print(f'💾 Loading data to databricks table: {target_full_path}')\n",
    "try:\n",
    "    order_df.write \\\n",
    "        .format('delta') \\\n",
    "        .mode('overwrite') \\\n",
    "        .option('overwriteSchema', 'true') \\\n",
    "        .saveAsTable(target_full_path)\n",
    "    print(f'✅ Data loaded successfully to {target_full_path}')\n",
    "\n",
    "    result_count = spark.sql(f'SELECT COUNT(*) as count FROM {target_full_path}').collect()[0]['count']\n",
    "    print(f'📊 Records in target table: {result_count}')\n",
    "    print(f'\\n📖 Sample from Silver table:')\n",
    "    \n",
    "    spark.sql(f'SELECT * FROM {target_full_path} ORDER BY OrderId').show(10, truncate=False)\n",
    "    print(f'🎉 Order data load complete!')\n",
    "except Exception as e:\n",
    "    print(f'❌ Error loading data to table: {str(e)}')\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
