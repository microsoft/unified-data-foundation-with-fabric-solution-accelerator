{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d15aadc",
   "metadata": {},
   "source": [
    "# Load Bronze Data to Silver Table - OrderLine (ADB Channel)\n",
    "\n",
    "## Overview\n",
    "Load OrderLine sample data from Bronze lakehouse CSV file into Silver table in Databricks workspace catalog.\n",
    "\n",
    "## Data Flow\n",
    "- **Source (CSV)**: `DBFS sales/OrderLine_Samples_ADB.csv`\n",
    "- **Target**: Workspace catalog table: `sales.orderline`\n",
    "- **Process**: Read CSV, validate schema, check data quality, load to Delta table, verify load\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b639182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c567de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for schema, table name, and file path\n",
    "dbutils.widgets.text(\"catalog_name\", \"maag_adb2\")\n",
    "dbutils.widgets.text(\"schema_name\", \"sales\")\n",
    "dbutils.widgets.text(\"base_path\", \"/FileStore/tables/sales\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "table_orderline = 'orderline'\n",
    "orderline_csv = f'{base_path}/OrderLine_Samples_ADB.csv'\n",
    "target_full_path = f'{catalog_name}.{schema_name}.{table_orderline}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DataFrame\n",
    "print(f'ğŸ”„ Loading OrderLine data')\n",
    "print(f'ğŸ“‚ Source: {orderline_csv}')\n",
    "print(f'ğŸ¯ Target: {target_full_path}')\n",
    "\n",
    "orderline_df = spark.read.option('header', True).option('inferSchema', True).csv(orderline_csv)\n",
    "\n",
    "print(f'âœ… Data loaded successfully')\n",
    "print(f'ğŸ“Š Records: {orderline_df.count()}')\n",
    "print(f'ğŸ“‹ Columns: {orderline_df.columns}')\n",
    "\n",
    "# Display sample data from dataframe\n",
    "print(f'ğŸ“– Sample data:')\n",
    "orderline_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4055a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and confirm to target schema\n",
    "print(f'ğŸ” Validating data quality...')\n",
    "required_columns = [\n",
    "    'OrderLineId', 'OrderId', 'ProductId', 'Quantity', 'UnitPrice', 'LineTotal', 'CreatedBy'\n",
    "]\n",
    "missing_columns = [c for c in required_columns if c not in orderline_df.columns]\n",
    "if missing_columns:\n",
    "    print(f'âš ï¸ Warning: Missing columns in source data: {missing_columns}')\n",
    "else:\n",
    "    print(f'âœ… All required columns present in source data.')\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "for col_name in missing_columns:\n",
    "    if col_name in ['Quantity', 'UnitPrice', 'LineTotal']:\n",
    "        orderline_df = orderline_df.withColumn(col_name, F.lit(0.0))\n",
    "    elif col_name == 'CreatedBy':\n",
    "        orderline_df = orderline_df.withColumn(col_name, F.lit('Script'))\n",
    "    else:\n",
    "        orderline_df = orderline_df.withColumn(col_name, F.lit(''))\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "orderline_df = orderline_df.withColumn('OrderLineId', col('OrderLineId').cast(StringType()))\n",
    "orderline_df = orderline_df.withColumn('OrderId', col('OrderId').cast(StringType()))\n",
    "orderline_df = orderline_df.withColumn('ProductId', col('ProductId').cast(StringType()))\n",
    "orderline_df = orderline_df.withColumn('Quantity', col('Quantity').cast(DoubleType()))\n",
    "orderline_df = orderline_df.withColumn('UnitPrice', col('UnitPrice').cast(DoubleType()))\n",
    "orderline_df = orderline_df.withColumn('LineTotal', col('LineTotal').cast(DoubleType()))\n",
    "orderline_df = orderline_df.withColumn('CreatedBy', col('CreatedBy').cast(StringType()))\n",
    "orderline_df = orderline_df.select(required_columns)\n",
    "\n",
    "print(f'ğŸ“Š Data Quality Check:')\n",
    "null_counts = orderline_df.select([F.sum(col(c).isNull().cast('int')).alias(c) for c in required_columns]).collect()[0]\n",
    "for col_name in required_columns:\n",
    "    null_count = null_counts[col_name]\n",
    "    if null_count > 0:\n",
    "        print(f'  {col_name}: {null_count} null values')\n",
    "    else:\n",
    "        print(f'  {col_name}: âœ… No nulls')\n",
    "\n",
    "print(f'ğŸ¯ ProductId Distribution:')\n",
    "orderline_df.groupBy('ProductId').count().orderBy('ProductId').show()\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema_name}')\n",
    "print(f'ğŸ’¾ Loading data to databricks table: {target_full_path}')\n",
    "\n",
    "try:\n",
    "    orderline_df.write \\\n",
    "        .format('delta') \\\n",
    "        .mode('overwrite') \\\n",
    "        .option('overwriteSchema', 'true') \\\n",
    "        .saveAsTable(target_full_path)\n",
    "    print(f'âœ… Data loaded successfully to {target_full_path}')\n",
    "\n",
    "    result_count = spark.sql(f'SELECT COUNT(*) as count FROM {target_full_path}').collect()[0]['count']\n",
    "    print(f'ğŸ“Š Records in target table: {result_count}')\n",
    "    print(f'\\nğŸ“– Sample from Silver table:')\n",
    "\n",
    "    spark.sql(f'SELECT * FROM {target_full_path} ORDER BY OrderLineId').show(10, truncate=False)\n",
    "    print(f'ğŸ‰ OrderLine data load complete!')\n",
    "except Exception as e:\n",
    "    print(f'âŒ Error loading data to table: {str(e)}')\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
