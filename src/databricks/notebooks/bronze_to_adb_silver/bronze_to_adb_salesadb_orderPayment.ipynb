{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d560c95",
   "metadata": {},
   "source": [
    "# Load Bronze Data to Silver Table - OrderPayment (ADB Channel)\n",
    "\n",
    "## Overview\n",
    "Load OrderPayment sample data from Bronze lakehouse CSV file into Silver table in Databricks workspace catalog.\n",
    "\n",
    "## Data Flow\n",
    "- **Source (CSV)**: `DBFS sales/OrderPayment_ADB.csv`\n",
    "- **Target**: Workspace catalog table: `sales.orderpayment`\n",
    "- **Process**: Import libraries, define variables, read CSV, validate schema, check data quality, load to Delta table, verify load\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for schema, table name, and file path\n",
    "dbutils.widgets.text(\"catalog_name\", \"maag_adb2\")\n",
    "dbutils.widgets.text(\"schema_name\", \"sales\")\n",
    "dbutils.widgets.text(\"base_path\", \"/FileStore/tables/sales\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "\n",
    "table_orderpayment = 'orderpayment'\n",
    "orderpayment_csv = f'{base_path}/OrderPayment_ADB.csv'\n",
    "target_full_path = f'{catalog_name}.{schema_name}.{table_orderpayment}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210de6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DataFrame\n",
    "print(f'🔄 Loading OrderPayment data')\n",
    "print(f'📂 Source: {orderpayment_csv}')\n",
    "print(f'🎯 Target: {target_full_path}')\n",
    "\n",
    "orderpayment_df = spark.read.option('header', True).option('inferSchema', True).csv(orderpayment_csv)\n",
    "\n",
    "print(f'✅ Data loaded successfully')\n",
    "print(f'📊 Records: {orderpayment_df.count()}')\n",
    "print(f'📋 Columns: {orderpayment_df.columns}')\n",
    "\n",
    "# Display sample data from dataframe\n",
    "print(f'📖 Sample data:')\n",
    "orderpayment_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccbe34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and conform to target schema\n",
    "print(f'🔍 Validating data quality...')\n",
    "required_columns = [\n",
    "    'OrderId', 'PaymentMethod', 'TransactionId'\n",
    "]\n",
    "missing_columns = [c for c in required_columns if c not in orderpayment_df.columns]\n",
    "if missing_columns:\n",
    "    print(f'⚠️ Warning: Missing columns in source data: {missing_columns}')\n",
    "else:\n",
    "    print(f'✅ All required columns present in source data.')\n",
    "from pyspark.sql import functions as F\n",
    "for col_name in missing_columns:\n",
    "    orderpayment_df = orderpayment_df.withColumn(col_name, F.lit(''))\n",
    "from pyspark.sql.types import StringType\n",
    "orderpayment_df = orderpayment_df.withColumn('OrderId', col('OrderId').cast(StringType()))\n",
    "orderpayment_df = orderpayment_df.withColumn('PaymentMethod', col('PaymentMethod').cast(StringType()))\n",
    "orderpayment_df = orderpayment_df.withColumn('TransactionId', col('TransactionId').cast(StringType()))\n",
    "orderpayment_df = orderpayment_df.select(required_columns)\n",
    "\n",
    "print(f'📊 Data Quality Check:')\n",
    "null_counts = orderpayment_df.select([F.sum(col(c).isNull().cast('int')).alias(c) for c in required_columns]).collect()[0]\n",
    "for col_name in required_columns:\n",
    "    null_count = null_counts[col_name]\n",
    "    if null_count > 0:\n",
    "        print(f'  {col_name}: {null_count} null values')\n",
    "    else:\n",
    "        print(f'  {col_name}: ✅ No nulls')\n",
    "\n",
    "print(f'🎯 PaymentMethod Distribution:')\n",
    "orderpayment_df.groupBy('PaymentMethod').count().orderBy('PaymentMethod').show()\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema_name}')\n",
    "print(f'💾 Loading data to databricks table: {target_full_path}')\n",
    "\n",
    "try:\n",
    "    orderpayment_df.write \\\n",
    "        .format('delta') \\\n",
    "        .mode('overwrite') \\\n",
    "        .option('overwriteSchema', 'true') \\\n",
    "        .saveAsTable(target_full_path)\n",
    "    print(f'✅ Data loaded successfully to {target_full_path}')\n",
    "\n",
    "    result_count = spark.sql(f'SELECT COUNT(*) as count FROM {target_full_path}').collect()[0]['count']\n",
    "    print(f'📊 Records in target table: {result_count}')\n",
    "    print(f'\\n📖 Sample from Silver table:')\n",
    "\n",
    "    spark.sql(f'SELECT * FROM {target_full_path} ORDER BY OrderId').show(10, truncate=False)\n",
    "    print(f'🎉 OrderPayment data load complete!')\n",
    "except Exception as e:\n",
    "    print(f'❌ Error loading data to table: {str(e)}')\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
